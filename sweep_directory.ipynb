{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing with sweep_directory function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using 3 subjects from HNU1 for testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# Copyright 2016 NeuroData (http://neurodata.io)\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#\n",
    "\n",
    "# bids.py\n",
    "# Created by Eric Bridgeford on 2017-08-09.\n",
    "# Email: ebridge2@jhu.edu\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "from bids import BIDSLayout\n",
    "import re\n",
    "from itertools import product\n",
    "import boto3\n",
    "from ndmg.utils import gen_utils as mgu\n",
    "import os\n",
    "\n",
    "\n",
    "class name_resource:\n",
    "    \"\"\"\n",
    "    A class for naming derivatives under the BIDs spec.\n",
    "    \"\"\"\n",
    "    def __init__(self, modf, t1wf, tempf, opath):\n",
    "        self.__subi__ = os.path.basename(modf).split('.')[0]\n",
    "        self.__anati__ = os.path.basename(t1wf).split('.')[0]\n",
    "        self.__sub__ = re.search(r'(sub-)(?!.*sub-).*?(?=[_])', modf).group()\n",
    "        self.__suball__  = \"sub-{}\".format(self.__sub__)\n",
    "        self.__ses__ = re.search(r'(ses-)(?!.*ses-).*?(?=[_])', modf)\n",
    "        if self.__ses__:\n",
    "            self.__ses__ = self.__ses__.group()\n",
    "            self.__suball__ = self.__suball__ + \"_ses-{}\".format(self.__ses__)\n",
    "        self.__run__ = re.search(r'(run-)(?!.*run-).*?(?=[_])', modf)\n",
    "        if self.__run__:\n",
    "            self.__run__ = self.__run__.group()\n",
    "            self.__suball__ = self.__suball__ + \"_run-{}\".format(self.__run__)\n",
    "        self.__task__ = re.search(r'(task-)(?!.*task-).*?(?=[_])', modf)\n",
    "        if self.__task__:\n",
    "            self.__task__ = self.__task__.group()\n",
    "            self.__suball__ = self.__suball__ + \"_run-{}\".format(self.__task__)\n",
    "        self.__temp__ = os.path.basename(tempf).split('.')[0]\n",
    "        self.__space__ = re.split(r'[._]', self.__temp__)[0]\n",
    "        self.__res__ = re.search(r'(res-)(?!.*res-).*?(?=[_])', tempf)\n",
    "        if self.__res__:\n",
    "            self.__res__ = self.__res__.group()\n",
    "        self.__basepath__ = opath\n",
    "        self.__outdir__ = self._get_outdir()\n",
    "        return\n",
    "\n",
    "    def add_dirs(self, paths, labels, label_dirs):\n",
    "        \"\"\"\n",
    "        creates tmp and permanent directories for the desired suffixes.\n",
    "\n",
    "        **Positional Arguments:\n",
    "            - paths:\n",
    "                - a dictionary of keys to suffix directories desired.\n",
    "        \"\"\"\n",
    "        self.dirs = {}\n",
    "        if not isinstance(labels, list):\n",
    "            labels = [labels]\n",
    "        dirtypes = ['output', 'tmp', 'qa']\n",
    "        for dirt in dirtypes:\n",
    "            olist = [self.get_outdir()]\n",
    "            self.dirs[dirt] = {}\n",
    "            if dirt in ['tmp', 'qa']:\n",
    "                olist = olist +[dirt] + self.get_sub_info()\n",
    "            self.dirs[dirt]['base'] = os.path.join(*olist)\n",
    "            for kwd, path in paths.iteritems():\n",
    "                newdir = os.path.join(*[self.dirs[dirt]['base'], path])\n",
    "                if kwd in label_dirs:  # levels with label granularity\n",
    "                    self.dirs[dirt][kwd] = {}\n",
    "                    for label in labels:\n",
    "                        labname = self.get_label(label)\n",
    "                        self.dirs[dirt][kwd][labname] = os.path.join(newdir,\n",
    "                           labname)\n",
    "                else:\n",
    "                    self.dirs[dirt][kwd] = newdir\n",
    "        newdirs = flatten(self.dirs, [])\n",
    "        cmd = \"mkdir -p {}\".format(\" \".join(newdirs))\n",
    "        mgu.execute_cmd(cmd)  # make the directories\n",
    "        return\n",
    "\n",
    "    def add_dirs_dwi(namer, paths, labels, label_dirs):\n",
    "        \"\"\"\n",
    "        creates tmp and permanent directories for the desired suffixes.\n",
    "\n",
    "        **Positional Arguments:\n",
    "            - paths:\n",
    "                - a dictionary of keys to suffix directories desired.\n",
    "        \"\"\"\n",
    "        namer.dirs = {}\n",
    "        if not isinstance(labels, list):\n",
    "            labels = [labels]\n",
    "        dirtypes = ['output']\n",
    "        for dirt in dirtypes:\n",
    "            olist = [namer.get_outdir()]\n",
    "            namer.dirs[dirt] = {}\n",
    "            if dirt in ['tmp']:\n",
    "                olist = olist +[dirt]\n",
    "            namer.dirs[dirt]['base'] = os.path.join(*olist)\n",
    "            for kwd, path in paths.iteritems():\n",
    "                newdir = os.path.join(*[namer.dirs[dirt]['base'], path])\n",
    "                if kwd in label_dirs:  # levels with label granularity\n",
    "                    namer.dirs[dirt][kwd] = {}\n",
    "                    for label in labels:\n",
    "                        labname = namer.get_label(label)\n",
    "                        namer.dirs[dirt][kwd][labname] = os.path.join(newdir,\n",
    "                           labname)\n",
    "                else:\n",
    "                    namer.dirs[dirt][kwd] = newdir\n",
    "        namer.dirs['tmp'] = {}\n",
    "        namer.dirs['tmp']['base'] = namer.get_outdir() + '/tmp'\n",
    "        namer.dirs['tmp']['reg_a'] = namer.dirs['tmp']['base'] + '/reg_a'\n",
    "        namer.dirs['tmp']['reg_m'] = namer.dirs['tmp']['base'] + '/reg_m'\n",
    "        namer.dirs['qa'] = {}\n",
    "        namer.dirs['qa']['base'] = namer.get_outdir() + '/qa'\n",
    "        namer.dirs['qa']['adjacency'] = namer.dirs['qa']['base'] + '/adjacency'\n",
    "        namer.dirs['qa']['fibers'] = namer.dirs['qa']['base'] + '/fibers'\n",
    "        namer.dirs['qa']['graphs'] = namer.dirs['qa']['base'] + '/graphs'\n",
    "        namer.dirs['qa']['graphs_plotting'] = namer.dirs['qa']['base'] + '/graphs_plotting'\n",
    "        namer.dirs['qa']['mri'] = namer.dirs['qa']['base'] + '/mri'\n",
    "        namer.dirs['qa']['reg'] = namer.dirs['qa']['base'] + '/reg'\n",
    "        namer.dirs['qa']['tensor'] = namer.dirs['qa']['base'] + '/tensor'\n",
    "        newdirs = flatten(namer.dirs, [])\n",
    "        cmd = \"mkdir -p {}\".format(\" \".join(newdirs))\n",
    "        mgu.execute_cmd(cmd)  # make the directories\n",
    "        return\n",
    "        \n",
    "    def _get_outdir(self):\n",
    "        \"\"\"\n",
    "        Called by constructor to initialize the output directory.\n",
    "        \"\"\"\n",
    "        olist = [self.__basepath__]\n",
    "        #olist.append(self.__sub__)\n",
    "        #if self.__ses__:\n",
    "        #    olist.append(self.__ses__)\n",
    "        return os.path.join(*olist)\n",
    "\n",
    "    def get_outdir(self):\n",
    "        \"\"\"\n",
    "        Returns the base  output directory for a particular subject\n",
    "        (+ appropriate granularity).\n",
    "        \"\"\"\n",
    "        return self.__outdir__\n",
    "\n",
    "    def get_template_info(self):\n",
    "        \"\"\"\n",
    "        returns the formatted spatial information associated with a template.-\n",
    "        \"\"\"\n",
    "        return \"space-{}_{}\".format(self.__space__, self.__res__)\n",
    "\n",
    "    def get_template_space(self):\n",
    "        return \"space-{}_{}\".format(self.__space__, self.__res__)\n",
    "    \n",
    "    def get_label(self, label):\n",
    "        \"\"\"\n",
    "        return the formatted label information for the parcellation.\n",
    "        \"\"\"\n",
    "        return mgu.get_filename(label)\n",
    "        #return \"label-{}\".format(re.split(r'[._]',\n",
    "        #                         os.path.basename(label))[0])\n",
    "\n",
    "    def name_derivative(self, folder, derivative):\n",
    "        \"\"\"\n",
    "        names a particular derivative by the following spec:\n",
    "\n",
    "        self.__opath__/mod/type/[specific/]derivative\n",
    "\n",
    "        ***Positional Arguments:**\n",
    "\n",
    "            derivative:\n",
    "                - the name of the file to be produced.\n",
    "            mod:\n",
    "                - the modality. Should be a BIDs-compliant name (bold, t1w,\n",
    "                anat).\n",
    "            type:\n",
    "                - the inner directory to place the file.\n",
    "            specific:\n",
    "                - an additional, optional layer of granularity.\n",
    "        \"\"\"\n",
    "        return os.path.join(*[folder, derivative])\n",
    "\n",
    "    def get_mod_source(self):\n",
    "        return self.__subi__\n",
    "\n",
    "    def get_anat_source(self):\n",
    "        return self.__anati__\n",
    "\n",
    "    def get_sub_info(self):\n",
    "        olist = []\n",
    "        if self.__sub__:\n",
    "            olist.append(self.__sub__)\n",
    "        if self.__ses__:\n",
    "            olist.append(self.__ses__)\n",
    "        return olist\n",
    "\n",
    "def flatten(current, result=[]):\n",
    "    if isinstance(current, dict):\n",
    "        for key in current:\n",
    "            flatten(current[key], result)\n",
    "    else:\n",
    "        result.append(current)\n",
    "    return result\n",
    "\n",
    "def sweep_directory(bdir, subj=None, sesh=None, task=None, run=None, modality='dwi'):\n",
    "    \"\"\"\n",
    "    Given a BIDs formatted directory, crawls the BIDs dir and prepares the\n",
    "    necessary inputs for the NDMG pipeline. Uses regexes to check matches for\n",
    "    BIDs compliance.\n",
    "    \"\"\"\n",
    "    if modality == 'dwi':\n",
    "        dwis = []\n",
    "        bvals = []\n",
    "        bvecs = []\n",
    "    elif modality == 'func':\n",
    "        funcs = []\n",
    "    anats = []\n",
    "    layout = BIDSLayout(bdir)  # initialize BIDs tree on bdir\n",
    "    # get all files matching the specific modality we are using\n",
    "    if subj is None:\n",
    "        subjs = layout.get_subjects()  # list of all the subjects\n",
    "    else:\n",
    "        subjs = as_list(subj)  # make it a list so we can iterate\n",
    "    for sub in subjs:\n",
    "        if not sesh:\n",
    "            seshs = layout.get_sessions(subject=sub)\n",
    "            seshs += [None]  # in case there are non-session level inputs\n",
    "        else:\n",
    "            seshs = as_list(sesh)  # make a list so we can iterate\n",
    "\n",
    "        if not task:\n",
    "            tasks = layout.get_tasks(subject=sub)\n",
    "            tasks += [None]\n",
    "        else:\n",
    "            tasks = as_list(task)\n",
    "\n",
    "        if not run:\n",
    "            runs = layout.get_runs(subject=sub)\n",
    "            runs += [None]\n",
    "        else:\n",
    "            runs = as_list(run)\n",
    "\n",
    "        # all the combinations of sessions and tasks that are possible\n",
    "        for (ses, tas, ru) in product(seshs, tasks, runs):\n",
    "            # the attributes for our modality img\n",
    "            mod_attributes = [sub, ses, tas, ru]\n",
    "            # the keys for our modality img\n",
    "            mod_keys = ['subject', 'session', 'task', 'run']\n",
    "            # our query we will use for each modality img\n",
    "            mod_query = {'modality': modality}\n",
    "            if modality == 'dwi':\n",
    "                type_img = 'dwi'  # use the dwi image\n",
    "            elif modality == 'func':\n",
    "                type_img = 'bold'  # use the bold image\n",
    "            mod_query['type'] = type_img\n",
    "\n",
    "            for attr, key in zip(mod_attributes, mod_keys):\n",
    "                if attr:\n",
    "                    mod_query[key] = attr\n",
    "\n",
    "            anat_attributes = [sub, ses]  # the attributes for our anat img\n",
    "            anat_keys = ['subject', 'session']  # the keys for our modality img\n",
    "            # our query for the anatomical image\n",
    "            anat_query = {'modality': 'anat', 'type': 'T1w',\n",
    "                          'extensions': 'nii.gz|nii'}\n",
    "            for attr, key in zip(anat_attributes, anat_keys):\n",
    "                if attr:\n",
    "                    anat_query[key] = attr\n",
    "            # make a query to fine the desired files from the BIDSLayout\n",
    "            anat = layout.get(**anat_query)\n",
    "            if modality == 'dwi':\n",
    "                dwi = layout.get(**merge_dicts(mod_query,\n",
    "                                               {'extensions': 'nii.gz|nii'}))\n",
    "                bval = layout.get(**merge_dicts(mod_query,\n",
    "                                                {'extensions': 'bval'}))\n",
    "                bvec = layout.get(**merge_dicts(mod_query,\n",
    "                                                {'extensions': 'bvec'}))\n",
    "                if (anat and dwi and bval and bvec):\n",
    "                    for (dw, bva, bve) in zip(dwi, bval, bvec):\n",
    "                        if dw.filename not in dwis:\n",
    "                            # if all the required files exist, append by the first\n",
    "                            # match (0 index)\n",
    "                            anats.append(anat[0].filename)\n",
    "                            dwis.append(dw.filename)\n",
    "                            bvals.append(bva.filename)\n",
    "                            bvecs.append(bve.filename)\n",
    "            elif modality == 'func':\n",
    "                func = layout.get(**merge_dicts(mod_query,\n",
    "                                                {'extensions': 'nii.gz|nii'}))\n",
    "                if func and anat:\n",
    "                    for fun in func:\n",
    "                        if fun.filename not in funcs:\n",
    "                            funcs.append(fun.filename)\n",
    "                            anats.append(anat[0].filename)\n",
    "    if modality == 'dwi':\n",
    "        if not len(dwis) or not len(bvals) or not len(bvecs) or not len(anats):\n",
    "            print(\"No dMRI files found in BIDs spec. Skipping...\")\n",
    "        return (dwis, bvals, bvecs, anats)\n",
    "    elif modality == 'func':\n",
    "        if not len(funcs) or not len(anats):\n",
    "            print(\"No fMRI files found in BIDs spec. Skipping...\")\n",
    "        return (funcs, anats)\n",
    "    else:\n",
    "        raise ValueError('Incorrect modality passed.\\\n",
    "                         Choices are \\'func\\' and \\'dwi\\'.')\n",
    "\n",
    "\n",
    "def as_list(x):\n",
    "    \"\"\"\n",
    "    A function to convert an item to a list if it is not, or pass\n",
    "    it through otherwise.\n",
    "    \"\"\"\n",
    "    if not isinstance(x, list):\n",
    "        return [x]\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "\n",
    "def merge_dicts(x, y):\n",
    "    \"\"\"\n",
    "    A function to merge two dictionaries, making it easier for us to make\n",
    "    modality specific queries for dwi images (since they have variable\n",
    "    extensions due to having an nii.gz, bval, and bvec file).\n",
    "    \"\"\"\n",
    "    z = x.copy()\n",
    "    z.update(y)\n",
    "    return z\n",
    "\n",
    "def s3_get_data(bucket, remote, local, public=True):\n",
    "    \"\"\"\n",
    "    Given an s3 bucket, data location on the bucket, and a download location,\n",
    "    crawls the bucket and recursively pulls all data.\n",
    "    \"\"\"\n",
    "    client = boto3.client('s3')\n",
    "    if not public:\n",
    "        bkts = [bk['Name'] for bk in client.list_buckets()['Buckets']]\n",
    "        if bucket not in bkts:\n",
    "            sys.exit(\"Error: could not locate bucket. Available buckets: \" +\n",
    "                     \", \".join(bkts))\n",
    "\n",
    "    cmd = 'aws s3 cp --recursive s3://{}/{}/ {}'.format(bucket, remote, local)\n",
    "    if public:\n",
    "        cmd += ' --no-sign-request --region=us-east-1'\n",
    "\n",
    "    std, err = mgu.execute_cmd('mkdir -p {}'.format(local))\n",
    "    std, err = mgu.execute_cmd(cmd)\n",
    "\n",
    "\n",
    "def s3_push_data(bucket, remote, outDir, modifier, creds=True):\n",
    "    cmd = 'aws s3 cp --exclude \"tmp/*\" {} s3://{}/{}/{} --recursive --acl public-read'\n",
    "    cmd = cmd.format(outDir, bucket, remote, modifier)\n",
    "    if not creds:\n",
    "        print(\"Note: no credentials provided, may fail to push big files.\")\n",
    "        cmd += ' --no-sign-request'\n",
    "    mgu.execute_cmd(cmd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store outputs in a variable\n",
    "x = sweep_directory('HNU1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sweep_directory creates a 4-tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- x[0] --> dwi.nii.gz\n",
    "- x[1] --> dwi.bval\n",
    "- x[2] --> dwi.bvec\n",
    "- x[3] --> T1w.nii.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/alex/Dropbox/NeuroData/ndmg-top/exploration/aloftus/HNU1/sub-0025427/ses-1/dwi/sub-0025427_ses-1_dwi.nii.gz',\n",
       " '/Users/alex/Dropbox/NeuroData/ndmg-top/exploration/aloftus/HNU1/sub-0025427/ses-2/dwi/sub-0025427_ses-2_dwi.nii.gz',\n",
       " '/Users/alex/Dropbox/NeuroData/ndmg-top/exploration/aloftus/HNU1/sub-0025427/ses-3/dwi/sub-0025427_ses-3_dwi.nii.gz',\n",
       " '/Users/alex/Dropbox/NeuroData/ndmg-top/exploration/aloftus/HNU1/sub-0025427/ses-4/dwi/sub-0025427_ses-4_dwi.nii.gz',\n",
       " '/Users/alex/Dropbox/NeuroData/ndmg-top/exploration/aloftus/HNU1/sub-0025427/ses-5/dwi/sub-0025427_ses-5_dwi.nii.gz']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0][:5]  # all the dwi.nii.gz files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/alex/Dropbox/NeuroData/ndmg-top/exploration/aloftus/HNU1/sub-0025427/ses-1/dwi/sub-0025427_ses-1_dwi.bval',\n",
       " '/Users/alex/Dropbox/NeuroData/ndmg-top/exploration/aloftus/HNU1/sub-0025427/ses-2/dwi/sub-0025427_ses-2_dwi.bval',\n",
       " '/Users/alex/Dropbox/NeuroData/ndmg-top/exploration/aloftus/HNU1/sub-0025427/ses-3/dwi/sub-0025427_ses-3_dwi.bval',\n",
       " '/Users/alex/Dropbox/NeuroData/ndmg-top/exploration/aloftus/HNU1/sub-0025427/ses-4/dwi/sub-0025427_ses-4_dwi.bval',\n",
       " '/Users/alex/Dropbox/NeuroData/ndmg-top/exploration/aloftus/HNU1/sub-0025427/ses-5/dwi/sub-0025427_ses-5_dwi.bval']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1][:5]  # all the *.bval files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/alex/Dropbox/NeuroData/ndmg-top/exploration/aloftus/HNU1/sub-0025427/ses-1/dwi/sub-0025427_ses-1_dwi.bvec',\n",
       " '/Users/alex/Dropbox/NeuroData/ndmg-top/exploration/aloftus/HNU1/sub-0025427/ses-2/dwi/sub-0025427_ses-2_dwi.bvec',\n",
       " '/Users/alex/Dropbox/NeuroData/ndmg-top/exploration/aloftus/HNU1/sub-0025427/ses-3/dwi/sub-0025427_ses-3_dwi.bvec',\n",
       " '/Users/alex/Dropbox/NeuroData/ndmg-top/exploration/aloftus/HNU1/sub-0025427/ses-4/dwi/sub-0025427_ses-4_dwi.bvec',\n",
       " '/Users/alex/Dropbox/NeuroData/ndmg-top/exploration/aloftus/HNU1/sub-0025427/ses-5/dwi/sub-0025427_ses-5_dwi.bvec']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[2][:5]  # all the *.bvec files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/alex/Dropbox/NeuroData/ndmg-top/exploration/aloftus/HNU1/sub-0025427/ses-1/anat/sub-0025427_ses-1_T1w.nii.gz',\n",
       " '/Users/alex/Dropbox/NeuroData/ndmg-top/exploration/aloftus/HNU1/sub-0025427/ses-2/anat/sub-0025427_ses-2_T1w.nii.gz',\n",
       " '/Users/alex/Dropbox/NeuroData/ndmg-top/exploration/aloftus/HNU1/sub-0025427/ses-3/anat/sub-0025427_ses-3_T1w.nii.gz',\n",
       " '/Users/alex/Dropbox/NeuroData/ndmg-top/exploration/aloftus/HNU1/sub-0025427/ses-4/anat/sub-0025427_ses-4_T1w.nii.gz',\n",
       " '/Users/alex/Dropbox/NeuroData/ndmg-top/exploration/aloftus/HNU1/sub-0025427/ses-5/anat/sub-0025427_ses-5_T1w.nii.gz']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[3][:5]  # all the t1w.nii.gz files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total sessions in HNU1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ndmg_env",
   "language": "python",
   "name": "ndmg_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
